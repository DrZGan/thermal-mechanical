{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "111ba704",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as np\n",
    "import numpy as onp\n",
    "from jax.flatten_util import ravel_pytree\n",
    "from functools import partial\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.optimize as opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df8313be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# About the decorator: f is a function (not JAX array) and JAX needs to know that so we specify it as \"static\"\n",
    "@partial(jax.jit, static_argnums=(2,))\n",
    "def rk4(state, t_crt, f, diff_args):\n",
    "    y_prev, t_prev = state\n",
    "    h = t_crt - t_prev\n",
    "    k1 = h * f(y_prev, t_prev, diff_args)\n",
    "    k2 = h * f(y_prev + k1/2., t_prev + h/2., diff_args)\n",
    "    k3 = h * f(y_prev + k2/2., t_prev + h/2., diff_args)\n",
    "    k4 = h * f(y_prev + k3, t_prev + h, diff_args)\n",
    "    y_crt = y_prev + 1./6 * (k1 + 2 * k2 + 2 * k3 + k4)\n",
    "    return (y_crt, t_crt), y_crt\n",
    "\n",
    "def Lorenz_rhs_func(state, t, diff_args):\n",
    "    rho, sigma, beta = diff_args[0]\n",
    "    x, y, z = state\n",
    "    return np.array([sigma * (y - x), x * (rho - z) - y, x * y - beta * z])\n",
    "\n",
    "def odeint(stepper, f, y0, ts, diff_args):\n",
    "\n",
    "    def stepper_partial(state, t_crt):\n",
    "        return stepper(state, t_crt, f, diff_args)\n",
    "\n",
    "    ys = []\n",
    "    state = (y0, ts[0])\n",
    "    for (i, t_crt) in enumerate(ts[1:]):\n",
    "        state, y = stepper_partial(state, t_crt)\n",
    "\n",
    "        # if i % 100 == 0:\n",
    "        #     print(f\"step {i}\")\n",
    "        #     if not np.all(np.isfinite(y)):\n",
    "        #         print(f\"Found np.inf or np.nan in y - stop the program\")      \n",
    "        #         exit()\n",
    "\n",
    "        ys.append(y)\n",
    "    ys = np.array(ys)\n",
    "    return ys\n",
    "\n",
    "\n",
    "def obj_func(yf, target_yf):\n",
    "    return np.sum((yf - target_yf)**2)\n",
    "\n",
    "\n",
    "def compute_gradient_finite_difference(y0, ts, obj_func, state_rhs_func, diff_args):\n",
    "    h = 1e-1\n",
    "    diff_args_flat, unravel = ravel_pytree(diff_args)\n",
    "\n",
    "    grads = []\n",
    "    for i in range(len(diff_args_flat)):\n",
    "        diff_args_flat_plus = jax.ops.index_add(diff_args_flat, i, h)\n",
    "        ys_plus_ = odeint(rk4, state_rhs_func, y0, ts, unravel(diff_args_flat_plus))\n",
    "        tau_plus = obj_func(ys_plus_[-1])\n",
    "\n",
    "        diff_args_flat_minus = jax.ops.index_add(diff_args_flat, i, -h)\n",
    "        ys_minus_ = odeint(rk4, state_rhs_func, y0, ts, unravel(diff_args_flat_minus))\n",
    "        tau_minus = obj_func(ys_minus_[-1])\n",
    "\n",
    "        grad = (tau_plus - tau_minus) / (2 * h)\n",
    "        grads.append(grad)\n",
    "\n",
    "    grads = unravel(np.array(grads))\n",
    "\n",
    "    print(f\"running finite difference\")\n",
    "\n",
    "    return grads\n",
    "\n",
    "def compute_gradient_ad(ys, ts, obj_func, state_rhs_func, diff_args):\n",
    "    rev_ts = ts[::-1]\n",
    "    rev_ys = ys[::-1]\n",
    "    y_bar, diff_args_bar = jax.grad(obj_func)(rev_ys[0]), jax.tree_map(np.zeros_like, diff_args)\n",
    "    y_bars = [y_bar]\n",
    "    diff_args_bars = [diff_args_bar]\n",
    "\n",
    "    @jax.jit\n",
    "    def adjoint_fn(y_prev, t_prev, t_crt, y_bar):\n",
    "        y_dot, vjpfun = jax.vjp(lambda y_prev, diff_args: rk4((y_prev, t_prev), t_crt, state_rhs_func, diff_args)[1], y_prev, diff_args)\n",
    "        y_bar, diff_args_bar = vjpfun(y_bar)  \n",
    "        return y_bar, diff_args_bar   \n",
    "\n",
    "    for i in range(len(ts) - 1):\n",
    "        y_prev = rev_ys[i + 1]\n",
    "        t_prev = rev_ts[i + 1]\n",
    "        y_crt = rev_ys[i]\n",
    "        t_crt = rev_ts[i]\n",
    "\n",
    "        y_bar, diff_args_bar = adjoint_fn(y_prev, t_prev, t_crt, y_bar)\n",
    "\n",
    "        y_bars.append(y_bar)\n",
    "        diff_args_bars.append(diff_args_bar)\n",
    "\n",
    "        if i % 100 == 0:\n",
    "            print(f\"Reverse step {i}\") \n",
    "            if not np.all(np.isfinite(y_bar)):\n",
    "                print(f\"Found np.inf or np.nan in y - stop the program\")             \n",
    "                exit()\n",
    "\n",
    "    y_bars = np.stack(y_bars)\n",
    "    grads = jax.tree_multimap(lambda *xs: np.sum(np.stack(xs), axis=0), *diff_args_bars)\n",
    "\n",
    "\n",
    "    print(f\"running autodiff\")\n",
    "\n",
    "    return grads\n",
    "\n",
    "\n",
    "\n",
    "def optimize(diff_args_0, y0, ts, obj_func, state_rhs_func, bounds=None):\n",
    "    x_ini, unravel = ravel_pytree(diff_args_0)\n",
    "    obj_vals = []\n",
    "\n",
    "    def objective(x):\n",
    "        print(f\"\\n######################### Evaluating objective value - step {objective.counter}\")\n",
    "        diff_args = unravel(x)\n",
    "        ys_ = odeint(rk4, state_rhs_func, y0, ts, diff_args)\n",
    "        ys = np.vstack((y0[None, ...], ys_))\n",
    "        obj_val = obj_func(ys[-1])\n",
    "\n",
    "        objective.diff_args = diff_args\n",
    "        objective.ys = ys\n",
    "        objective.x = diff_args\n",
    "\n",
    "        objective.counter += 1\n",
    "        obj_vals.append(obj_val)\n",
    "\n",
    "        print(f\"obj_val = {obj_val}\")\n",
    "        print(f\"diff_args = {diff_args}\")\n",
    "        print(f\"ys[-1] = {ys[-1]}\")\n",
    "\n",
    "        return obj_val\n",
    "\n",
    "    def derivative(x):\n",
    "        diff_args = objective.diff_args\n",
    "        ys = objective.ys\n",
    "\n",
    "        grads = compute_gradient_finite_difference(y0, ts, obj_func, state_rhs_func, diff_args)\n",
    "        print(f\"########################################################\")\n",
    "        print(f\"grads = {grads}\")\n",
    "        print(f\"########################################################\")\n",
    "\n",
    "        grads = compute_gradient_ad(ys, ts, obj_func, state_rhs_func, diff_args)\n",
    "        print(f\"########################################################\")\n",
    "        print(f\"grads = {grads}\")\n",
    "        print(f\"########################################################\")\n",
    "\n",
    "        grads_ravelled, _ = ravel_pytree(grads)\n",
    "        # 'L-BFGS-B' requires the following conversion, otherwise we get an error message saying\n",
    "        # -- input not fortran contiguous -- expected elsize=8 but got 4\n",
    "        return onp.array(grads_ravelled, order='F', dtype=onp.float64)\n",
    "\n",
    "    objective.counter = 0\n",
    "    options = {'maxiter': 1000, 'disp': True}  # CG or L-BFGS-B or Newton-CG or SLSQP\n",
    "    res = opt.minimize(fun=objective,\n",
    "                       x0=x_ini,\n",
    "                       method='SLSQP',\n",
    "                       jac=derivative,\n",
    "                       bounds=bounds,\n",
    "                       callback=None,\n",
    "                       options=options)\n",
    "\n",
    "    return objective.x\n",
    "\n",
    "\n",
    "def ground_truth(y0, ts):\n",
    "    sigma = 1e-3\n",
    "    key = jax.random.PRNGKey(0)\n",
    "    key, noises = split_and_sample(key, sigma, (len(ts) - 1,), 3)\n",
    "\n",
    "    diff_args_gt = [np.array([28., 10., 8./3.])]\n",
    "\n",
    "    ys_ = odeint(rk4, Lorenz_rhs_func, y0, ts, diff_args_gt)\n",
    "    ys = np.vstack((y0[None, ...], ys_))\n",
    "    print(ys[-1])\n",
    "    fig = plt.figure(figsize=(6, 4), dpi=150)\n",
    "    ax = fig.gca(projection='3d')\n",
    "    plot_3d_path(ax, ys, 'b')\n",
    "    return ys_[-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d150e309",
   "metadata": {},
   "outputs": [],
   "source": [
    "def exp():\n",
    "    y0 = np.array([1., 1., 1.])\n",
    "    dt = 1e-2\n",
    "    ts = np.arange(0, 101*dt, dt)\n",
    "    target_yf = ground_truth(y0, ts)\n",
    "\n",
    "    # diff_args_0 = [np.array([27.63452, 10.724965 , 8./3.])]\n",
    "    diff_args_0 = [np.array([26., 10., 2.])]\n",
    "    # diff_args_0 = [np.array([27.8, 10.3, 8./3.])]\n",
    "\n",
    "    # obj_func_partial = lambda yf: obj_func(yf, target_yf)\n",
    "    # bounds = np.array([[26., 30.], [8., 12.], [8./3., 8./3.]])\n",
    "    # optimize(diff_args_0, y0, ts, obj_func_partial, Lorenz_rhs_func, bounds)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fenics",
   "language": "python",
   "name": "fenics"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
